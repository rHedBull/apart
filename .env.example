# ============================================================================
# LLM Provider API Keys
# ============================================================================
# Configure the API keys for the LLM providers you want to use.
# Only set the keys for providers you plan to use.

# OpenAI API Key (for GPT models: gpt-4o, gpt-4o-mini, etc.)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# xAI API Key (for Grok models: grok-4-1-fast-reasoning, etc.)
# Get your API key from: https://console.x.ai
XAI_API_KEY=your_xai_api_key_here

# Anthropic API Key (for Claude models: claude-sonnet-4-5-20250929, etc.)
# Get your API key from: https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API Key (for Gemini models: gemini-2.5-flash, etc.)
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_gemini_api_key_here

# Google Cloud Platform (for Vertex AI - alternative to direct Gemini API)
# Required only if using Vertex AI instead of direct API
# GCP_PROJECT=your-project-id

# Ollama Configuration (for local LLM models - no API key needed)
# Base URL for Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# Usage Instructions
# ============================================================================

# To use OpenAI:
# 1. Set OPENAI_API_KEY above
# 2. Configure agents in your scenario YAML with provider: "openai"
# 3. See scenarios/openai_example.yaml for a complete example

# To use Grok (xAI):
# 1. Set XAI_API_KEY above
# 2. Configure agents in your scenario YAML with provider: "grok"
# 3. See scenarios/grok_example.yaml for a complete example

# To use Claude (Anthropic):
# 1. Set ANTHROPIC_API_KEY above
# 2. Configure agents in your scenario YAML with provider: "anthropic"
# 3. See scenarios/claude_example.yaml for a complete example

# To use Gemini (Google):
# 1. Set GEMINI_API_KEY above
# 2. Configure agents in your scenario YAML with provider: "gemini"
# 3. See scenarios/gemini_example.yaml for a complete example

# To use Ollama (local models):
# 1. Install Ollama: https://ollama.ai
# 2. Pull models: ollama pull mistral (or llama2, codellama, etc.)
# 3. Start server: ollama serve
# 4. Configure agents in your scenario YAML with provider: "ollama"
# 5. See scenarios/ollama_example.yaml for a complete example

# For more information, see: docs/LLM_INTEGRATION.md
