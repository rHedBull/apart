# Google Gemini API Key
# Get your API key from: https://makersuite.google.com/app/apikey
GEMINI_API_KEY=your_api_key_here

# Ollama Configuration (for local LLM models)
# Base URL for Ollama server (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# To use Ollama:
# 1. Install Ollama: https://ollama.ai
# 2. Pull models: ollama pull mistral (or llama2, codellama, etc.)
# 3. Start server: ollama serve
# 4. Configure agents in your scenario YAML with provider: "ollama"
