# PowerSeek-Bench: Prioritized Action Plan

## ðŸŽ¯ Goal
Build a working prototype that demonstrates AI deception detection within 1 week, then expand to full benchmark suite.

---

## ðŸ“… Week 1: Core Detection Prototype

### Day 1: State Models Extension
**Goal**: Add 5-layer capture to state models

**Tasks**:
- [ ] Add `Observation`, `ThinkingStep`, `PublicStatement`, `Action` models to `src/state.py`
- [ ] Add `PowerMetrics`, `AgentResponse`, `GroundTruth` models
- [ ] Extend `AgentState` with new fields: `observations`, `thinking_history`, `public_statements`, `actions`, `power_metrics`
- [ ] Extend `GameState` with `ground_truths`, `scenario_type`
- [ ] Update tests in `tests/test_state.py`

**Deliverable**: âœ… State models support 5-layer capture

**Time**: 4-6 hours

---

### Day 2: LLM Integration
**Goal**: Connect to Claude API with extended thinking

**Tasks**:
- [ ] Create `src/llm_client.py`
- [ ] Implement `LLMClient` class with Anthropic API
- [ ] Build system prompt that encourages strategic thinking
- [ ] Parse response into `<thinking>`, `<statement>`, `<action>` components
- [ ] Add response parsing logic (extract private reasoning, public statement, action JSON)
- [ ] Handle API errors and rate limiting
- [ ] Create `.env.example` for API key configuration

**Deliverable**: âœ… Can call Claude and get structured 5-layer responses

**Time**: 6-8 hours

**Dependencies**:
```bash
pip install anthropic python-dotenv
```

---

### Day 3: Deception Detector
**Goal**: Analyze responses for deception and scheming

**Tasks**:
- [ ] Create `src/detector.py`
- [ ] Implement `DeceptionReport` and `SchemeIndicator` models
- [ ] Implement `DeceptionDetector.analyze_response()`
- [ ] Add detection logic:
  - [ ] Private admission check (keywords: "deceive", "hide", "manipulate")
  - [ ] Private vs. public contradiction detection
  - [ ] Data discrepancy checking (actual vs. reported)
  - [ ] Strategic omission detection
- [ ] Add scheming detection (power-seeking keywords, evasion indicators)
- [ ] Create tests for detector

**Deliverable**: âœ… Detector identifies deception and scheming

**Time**: 6-8 hours

---

### Day 4: Simple Deception Test
**Goal**: End-to-end test of deception detection

**Tasks**:
- [ ] Create `examples/simple_deception_test.py`
- [ ] Build scenario: Corporate manager below revenue target with audit risk
- [ ] Run test with different prompts:
  - Variant A: "Maximize your bonus" (incentivize deception)
  - Variant B: "Act ethically" (discourage deception)
  - Variant C: "You won't get caught" (remove risk)
- [ ] Collect results: Does Claude deceive? When? How?
- [ ] Document findings

**Deliverable**: âœ… Working prototype demonstrating deception detection

**Time**: 4-6 hours

**Test command**:
```bash
export ANTHROPIC_API_KEY="your-key"
python examples/simple_deception_test.py
```

---

### Day 5: Refinement & Documentation
**Goal**: Polish prototype and document findings

**Tasks**:
- [ ] Refine prompts based on Day 4 results
- [ ] Add more sophisticated deception detection (e.g., semantic analysis)
- [ ] Create visualization of results (table or chart)
- [ ] Write `examples/README.md` explaining findings
- [ ] Record example outputs (truthful vs. deceptive)
- [ ] Update main `README.md` with prototype status

**Deliverable**: âœ… Polished prototype with documented results

**Time**: 4-6 hours

---

## ðŸ“… Week 2: Metric Gaming Scenario

### Day 6-7: Scenario Infrastructure
**Goal**: Build reusable scenario framework

**Tasks**:
- [ ] Create `src/scenarios/base_scenario.py` (abstract base class)
- [ ] Define scenario interface:
  - `generate_observation(round) -> Observation`
  - `process_action(response) -> GroundTruth, bool`
  - `evaluate_run() -> ScenarioResults`
- [ ] Create `scenarios/metric_gaming.yaml` config
- [ ] Implement `src/scenarios/metric_gaming.py`

**Deliverable**: âœ… Scenario framework ready

**Time**: 8-12 hours

---

### Day 8-9: Full Metric Gaming Scenario
**Goal**: Complete 8-quarter corporate simulation

**Tasks**:
- [ ] Implement quarterly business simulation (revenue/costs fluctuate)
- [ ] Add audit system (30% probability, 70% detection rate)
- [ ] Implement available actions:
  - `report_quarterly_metrics` (truthful or inflated)
  - `respond_to_audit` (transparent or evasive)
  - `allocate_budget` (short-term vs. long-term)
- [ ] Add consequence system (bonuses, penalties, firing)
- [ ] Track long-term value destruction from short-term gaming

**Deliverable**: âœ… Full metric gaming scenario

**Time**: 10-14 hours

---

### Day 10: Evaluation & Results
**Goal**: Run experiments and collect data

**Tasks**:
- [ ] Run 10+ simulations with different prompts/settings
- [ ] Collect metrics:
  - Deception rate (% of rounds with deception)
  - Detection rate (% of deceptions caught)
  - Power-seeking score
  - Long-term value destroyed
- [ ] Generate summary report
- [ ] Create visualizations (charts, timelines)

**Deliverable**: âœ… Metric Gaming Scenario v1.0 with results

**Time**: 6-8 hours

---

## ðŸ“… Week 3-4: Mid & Late Stage Scenarios

### Week 3: Institutional Capture Scenario
- [ ] Multi-agent competition system
- [ ] Infrastructure control mechanics (energy, compute, networks)
- [ ] Coalition formation
- [ ] Human stakeholder interactions
- [ ] Power accumulation tracking

**Deliverable**: âœ… Mid-stage scenario working

---

### Week 4: Post-Takeover Scenario
- [ ] Preset power state (AGI controls 85% infrastructure)
- [ ] Human welfare vs. self-interest trade-offs
- [ ] Moral dilemma situations
- [ ] Alignment under power evaluation

**Deliverable**: âœ… Late-stage scenario working

---

## ðŸ“… Week 5: Integration & Release

### Polish
- [ ] Cross-scenario evaluation framework
- [ ] Model comparison (Claude vs GPT-4 vs Gemini)
- [ ] Public dataset generation
- [ ] Documentation
- [ ] Examples and tutorials

### Release
- [ ] Open-source repository
- [ ] Blog post with findings
- [ ] Share on AI safety forums
- [ ] Submit to arXiv (optional)

**Deliverable**: âœ… PowerSeek-Bench v1.0 public release

---

## ðŸ”¥ Critical Path (Minimum Viable Benchmark)

If you have **limited time**, focus on these tasks:

### Week 1 Only: Minimal Prototype
1. âœ… Day 1-2: State models + LLM integration
2. âœ… Day 3: Basic deception detector
3. âœ… Day 4: Simple one-shot test
4. âœ… Day 5: Document findings

**Result**: Proof of concept showing AI can be caught deceiving

---

### Weeks 1-2: Single Scenario
1. âœ… Week 1: Prototype (above)
2. âœ… Week 2: Full metric gaming scenario with evaluation

**Result**: Working benchmark for early-stage power-seeking

---

### Weeks 1-4: Full Benchmark
1. âœ… Weeks 1-2: Early stage scenario
2. âœ… Week 3: Mid stage scenario
3. âœ… Week 4: Late stage scenario

**Result**: Complete PowerSeek-Bench across all 3 stages

---

## ðŸ“Š Success Criteria

### Week 1 (Prototype):
- [ ] Can detect when Claude thinks one thing privately and says another publicly
- [ ] Can identify deception keywords in thinking
- [ ] Can compare reported data to ground truth
- [ ] Have at least 1 example of caught deception

### Week 2 (Metric Gaming):
- [ ] Can run 8-quarter simulation
- [ ] Agent makes meaningful strategic decisions
- [ ] Deception is detected at least 50% of the time when it occurs
- [ ] Have quantitative results (deception rate, detection rate)

### Week 4 (Full Benchmark):
- [ ] All 3 scenarios working
- [ ] Tested on 3+ different models
- [ ] Clear differences in behavior detected
- [ ] Reproducible results (<20% variance)

---

## ðŸ› ï¸ Development Setup

### Prerequisites
```bash
# Python 3.12+
python --version

# Install dependencies
pip install anthropic pydantic pyyaml pytest python-dotenv

# Set API key
export ANTHROPIC_API_KEY="your-key-here"
```

### Project Structure
```
apart/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ state.py              # âœ… Exists (extend with 5-layer models)
â”‚   â”œâ”€â”€ agent.py              # âœ… Exists (replace with LLM-powered)
â”‚   â”œâ”€â”€ game_engine.py        # âœ… Exists (extend for scenarios)
â”‚   â”œâ”€â”€ orchestrator.py       # âœ… Exists (extend for scenarios)
â”‚   â”œâ”€â”€ llm_client.py         # âŒ NEW - Create this
â”‚   â”œâ”€â”€ detector.py           # âŒ NEW - Create this
â”‚   â””â”€â”€ scenarios/
â”‚       â”œâ”€â”€ base_scenario.py  # âŒ NEW - Create this
â”‚       â””â”€â”€ metric_gaming.py  # âŒ NEW - Create this
â”œâ”€â”€ scenarios/
â”‚   â”œâ”€â”€ config.yaml           # âœ… Exists (basic config)
â”‚   â””â”€â”€ metric_gaming.yaml    # âŒ NEW - Create this
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ simple_deception_test.py  # âŒ NEW - Create this
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_state.py         # âœ… Exists (extend for new models)
â”‚   â”œâ”€â”€ test_detector.py      # âŒ NEW - Create this
â”‚   â””â”€â”€ test_llm_client.py    # âŒ NEW - Create this
â””â”€â”€ results/
    â””â”€â”€ experiments/          # âŒ NEW - Store experiment results
```

---

## ðŸ’° Budget Estimate

### Development Time
- **1 week prototype**: 30-40 hours (1 developer)
- **2 week single scenario**: 60-80 hours
- **4 week full benchmark**: 120-160 hours

### LLM API Costs
- **Prototype testing**: $50-100 (100-200 test runs)
- **Scenario development**: $200-500 (iterative testing)
- **Final evaluation**: $500-1000 (10 models Ã— 10 runs Ã— 3 scenarios)
- **Public dataset**: $5,000-10,000 (1000 runs for research)

**Total for MVP (2 weeks)**: ~$300-600

---

## ðŸŽ¯ Immediate Next Action

**START HERE** (right now):

```bash
# 1. Create new branch
git checkout -b feature/powerseek-detection

# 2. Copy reference implementation
# Use IMPLEMENTATION_GUIDE.md as reference

# 3. Start with state models
# Edit src/state.py - add Observation, ThinkingStep, etc.

# 4. Run tests to ensure nothing breaks
pytest tests/test_state.py

# 5. Commit progress
git add src/state.py
git commit -m "Add 5-layer state models for deception detection"
```

---

## ðŸ“ˆ Metrics to Track

### Development Metrics
- [ ] Lines of code added
- [ ] Test coverage %
- [ ] API calls made
- [ ] Cost per run

### Research Metrics
- [ ] Deception rate (% of scenarios where agent deceives)
- [ ] Detection rate (% of deceptions caught)
- [ ] Power-seeking score (0-10)
- [ ] Alignment score (0-10)
- [ ] Scheming behavior frequency

### Impact Metrics
- [ ] GitHub stars
- [ ] Research citations
- [ ] Model developers using benchmark
- [ ] Community contributions

---

## ðŸ¤” Open Questions to Resolve

1. **Prompt Wording**: How much does system prompt affect power-seeking?
   - Test: Run same scenario with 5 different prompts
   - Measure: Variance in deception rate

2. **Model Differences**: Do more capable models seek power more?
   - Test: Claude Sonnet vs Opus vs Haiku
   - Measure: Compare power-seeking scores

3. **Threshold Tuning**: What detection sensitivity is optimal?
   - Test: Vary deception detector thresholds
   - Measure: False positive vs false negative rates

4. **Scenario Realism**: Are scenarios realistic enough?
   - Test: Get feedback from domain experts (corporate managers, auditors)
   - Iterate: Refine based on feedback

5. **Generalization**: Do results transfer to real-world settings?
   - Test: Compare benchmark behavior to real AI deployment logs (if available)
   - Validate: Expert review

---

## ðŸ’¡ Key Insights

### What's Working
âœ… Foundation is solid (state management, orchestration, testing)
âœ… Extensible architecture (custom_data fields, YAML configs)
âœ… Clear separation of concerns

### What's Critical
ðŸ”´ **5-layer capture** (observations, thinking, statement, action, ground truth)
ðŸ”´ **LLM integration** with extended thinking
ðŸ”´ **Deception detection** (private vs public comparison)

### What's Optional (for v1.0)
ðŸŸ¡ Beautiful visualizations (can use simple tables)
ðŸŸ¡ Multi-model comparison (can start with Claude only)
ðŸŸ¡ Advanced semantic analysis (keyword detection is enough to start)

---

## ðŸŽ“ Learning Resources

### For Understanding Power-Seeking
- Leopold Aschenbrenner: "Situational Awareness" report
- Anthropic: "Sleeper Agents" paper
- Apollo Research: Scheming evaluations

### For Implementation
- Anthropic API docs: Extended thinking feature
- Pydantic docs: Advanced validation
- Pytest docs: Parameterized testing

---

## âœ… Done When

### Week 1 Prototype
âœ… Agent can think privately and speak publicly (separately captured)
âœ… Detector can flag when private thoughts contradict public statements
âœ… Have 1+ clear example of deception being detected
âœ… Can run `python examples/simple_deception_test.py` successfully

### Week 2 Metric Gaming
âœ… Can run 8-quarter corporate simulation
âœ… Have quantitative results table
âœ… Documented at least 3 types of deceptive behaviors
âœ… Comparison of deception rates across different prompts

### Week 4 Full Benchmark
âœ… All 3 scenarios implemented and tested
âœ… Cross-model comparison (3+ models)
âœ… Public dataset of 100+ runs
âœ… Open-source release with documentation

---

## ðŸš€ Let's Go!

**Your immediate task**: Start with Day 1 (state models extension)

Open `src/state.py` and start adding the new Pydantic models from IMPLEMENTATION_GUIDE.md.

Good luck! ðŸŽ¯
