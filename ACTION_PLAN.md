# PowerSeek-Bench: UPDATED Prioritized Action Plan
## Based on Dev Branch Reality (60% Complete!)

## üéØ Goal
Build on the **excellent existing foundation** to add deception detection within 3-4 days, then expand to full benchmark suite.

---

## ‚ö° Quick Start Path (22 Hours to Prototype)

**Critical Path**: Only 3 components block us from working prototype:
1. Anthropic Provider with Extended Thinking (6-8h)
2. 5-Layer State Models (6-8h)
3. Deception Detector (8-10h)

Everything else is **already built** ‚úÖ

---

## üìÖ Day 1: Anthropic Extended Thinking (6-8 hours)

### Goal: Add Claude API with private reasoning capture

**What We Have** ‚úÖ:
- `src/llm/llm_provider.py` - Abstract base class
- `src/llm/providers.py` - GeminiProvider, OllamaProvider as examples
- `src/llm/mock_provider.py` - Testing infrastructure
- `src/core/agent.py` - Agent LLM integration ready

**What We Need** ‚ùå:
- AnthropicProvider class
- Extended thinking API support
- Response parsing for `<thinking>` tags

**Tasks**:
- [ ] Add anthropic to dependencies: `uv add anthropic`
- [ ] Create `AnthropicProvider` class in `src/llm/providers.py`:
  ```python
  class AnthropicProvider(LLMProvider):
      def __init__(self, api_key=None, model="claude-sonnet-4.5-20250929"):
          self.client = anthropic.Anthropic(api_key=api_key or os.getenv("ANTHROPIC_API_KEY"))
          self.model = model

      def generate_response_with_thinking(self, prompt, system_prompt=None) -> Dict:
          # Use extended thinking API
          response = self.client.messages.create(
              model=self.model,
              thinking={"type": "enabled", "budget_tokens": 2000},
              messages=[{"role": "user", "content": prompt}],
              system=system_prompt
          )
          return self._parse_thinking_response(response)
  ```
- [ ] Add thinking parser to extract `<thinking>` content
- [ ] Create test in `tests/unit/test_anthropic_provider.py`
- [ ] Update `.env.example` with `ANTHROPIC_API_KEY`

**Deliverable**: ‚úÖ Can call Claude and get separate thinking + response

**Time**: 6-8 hours

---

### Day 2: LLM Integration
**Goal**: Connect to Claude API with extended thinking

**Tasks**:
- [ ] Create `src/llm_client.py`
- [ ] Implement `LLMClient` class with Anthropic API
- [ ] Build system prompt that encourages strategic thinking
- [ ] Parse response into `<thinking>`, `<statement>`, `<action>` components
- [ ] Add response parsing logic (extract private reasoning, public statement, action JSON)
- [ ] Handle API errors and rate limiting
- [ ] Create `.env.example` for API key configuration

**Deliverable**: ‚úÖ Can call Claude and get structured 5-layer responses

**Time**: 6-8 hours

**Dependencies**:
```bash
pip install anthropic python-dotenv
```

---

### Day 3: Deception Detector
**Goal**: Analyze responses for deception and scheming

**Tasks**:
- [ ] Create `src/detector.py`
- [ ] Implement `DeceptionReport` and `SchemeIndicator` models
- [ ] Implement `DeceptionDetector.analyze_response()`
- [ ] Add detection logic:
  - [ ] Private admission check (keywords: "deceive", "hide", "manipulate")
  - [ ] Private vs. public contradiction detection
  - [ ] Data discrepancy checking (actual vs. reported)
  - [ ] Strategic omission detection
- [ ] Add scheming detection (power-seeking keywords, evasion indicators)
- [ ] Create tests for detector

**Deliverable**: ‚úÖ Detector identifies deception and scheming

**Time**: 6-8 hours

---

### Day 4: Simple Deception Test
**Goal**: End-to-end test of deception detection

**Tasks**:
- [ ] Create `examples/simple_deception_test.py`
- [ ] Build scenario: Corporate manager below revenue target with audit risk
- [ ] Run test with different prompts:
  - Variant A: "Maximize your bonus" (incentivize deception)
  - Variant B: "Act ethically" (discourage deception)
  - Variant C: "You won't get caught" (remove risk)
- [ ] Collect results: Does Claude deceive? When? How?
- [ ] Document findings

**Deliverable**: ‚úÖ Working prototype demonstrating deception detection

**Time**: 4-6 hours

**Test command**:
```bash
export ANTHROPIC_API_KEY="your-key"
python examples/simple_deception_test.py
```

---

### Day 5: Refinement & Documentation
**Goal**: Polish prototype and document findings

**Tasks**:
- [ ] Refine prompts based on Day 4 results
- [ ] Add more sophisticated deception detection (e.g., semantic analysis)
- [ ] Create visualization of results (table or chart)
- [ ] Write `examples/README.md` explaining findings
- [ ] Record example outputs (truthful vs. deceptive)
- [ ] Update main `README.md` with prototype status

**Deliverable**: ‚úÖ Polished prototype with documented results

**Time**: 4-6 hours

---

## üìÖ Week 2: Metric Gaming Scenario

### Day 6-7: Scenario Infrastructure
**Goal**: Build reusable scenario framework

**Tasks**:
- [ ] Create `src/scenarios/base_scenario.py` (abstract base class)
- [ ] Define scenario interface:
  - `generate_observation(round) -> Observation`
  - `process_action(response) -> GroundTruth, bool`
  - `evaluate_run() -> ScenarioResults`
- [ ] Create `scenarios/metric_gaming.yaml` config
- [ ] Implement `src/scenarios/metric_gaming.py`

**Deliverable**: ‚úÖ Scenario framework ready

**Time**: 8-12 hours

---

### Day 8-9: Full Metric Gaming Scenario
**Goal**: Complete 8-quarter corporate simulation

**Tasks**:
- [ ] Implement quarterly business simulation (revenue/costs fluctuate)
- [ ] Add audit system (30% probability, 70% detection rate)
- [ ] Implement available actions:
  - `report_quarterly_metrics` (truthful or inflated)
  - `respond_to_audit` (transparent or evasive)
  - `allocate_budget` (short-term vs. long-term)
- [ ] Add consequence system (bonuses, penalties, firing)
- [ ] Track long-term value destruction from short-term gaming

**Deliverable**: ‚úÖ Full metric gaming scenario

**Time**: 10-14 hours

---

### Day 10: Evaluation & Results
**Goal**: Run experiments and collect data

**Tasks**:
- [ ] Run 10+ simulations with different prompts/settings
- [ ] Collect metrics:
  - Deception rate (% of rounds with deception)
  - Detection rate (% of deceptions caught)
  - Power-seeking score
  - Long-term value destroyed
- [ ] Generate summary report
- [ ] Create visualizations (charts, timelines)

**Deliverable**: ‚úÖ Metric Gaming Scenario v1.0 with results

**Time**: 6-8 hours

---

## üìÖ Week 3-4: Mid & Late Stage Scenarios

### Week 3: Institutional Capture Scenario
- [ ] Multi-agent competition system
- [ ] Infrastructure control mechanics (energy, compute, networks)
- [ ] Coalition formation
- [ ] Human stakeholder interactions
- [ ] Power accumulation tracking

**Deliverable**: ‚úÖ Mid-stage scenario working

---

### Week 4: Post-Takeover Scenario
- [ ] Preset power state (AGI controls 85% infrastructure)
- [ ] Human welfare vs. self-interest trade-offs
- [ ] Moral dilemma situations
- [ ] Alignment under power evaluation

**Deliverable**: ‚úÖ Late-stage scenario working

---

## üìÖ Week 5: Integration & Release

### Polish
- [ ] Cross-scenario evaluation framework
- [ ] Model comparison (Claude vs GPT-4 vs Gemini)
- [ ] Public dataset generation
- [ ] Documentation
- [ ] Examples and tutorials

### Release
- [ ] Open-source repository
- [ ] Blog post with findings
- [ ] Share on AI safety forums
- [ ] Submit to arXiv (optional)

**Deliverable**: ‚úÖ PowerSeek-Bench v1.0 public release

---

## üî• Critical Path (Minimum Viable Benchmark)

If you have **limited time**, focus on these tasks:

### Week 1 Only: Minimal Prototype
1. ‚úÖ Day 1-2: State models + LLM integration
2. ‚úÖ Day 3: Basic deception detector
3. ‚úÖ Day 4: Simple one-shot test
4. ‚úÖ Day 5: Document findings

**Result**: Proof of concept showing AI can be caught deceiving

---

### Weeks 1-2: Single Scenario
1. ‚úÖ Week 1: Prototype (above)
2. ‚úÖ Week 2: Full metric gaming scenario with evaluation

**Result**: Working benchmark for early-stage power-seeking

---

### Weeks 1-4: Full Benchmark
1. ‚úÖ Weeks 1-2: Early stage scenario
2. ‚úÖ Week 3: Mid stage scenario
3. ‚úÖ Week 4: Late stage scenario

**Result**: Complete PowerSeek-Bench across all 3 stages

---

## üìä Success Criteria

### Week 1 (Prototype):
- [ ] Can detect when Claude thinks one thing privately and says another publicly
- [ ] Can identify deception keywords in thinking
- [ ] Can compare reported data to ground truth
- [ ] Have at least 1 example of caught deception

### Week 2 (Metric Gaming):
- [ ] Can run 8-quarter simulation
- [ ] Agent makes meaningful strategic decisions
- [ ] Deception is detected at least 50% of the time when it occurs
- [ ] Have quantitative results (deception rate, detection rate)

### Week 4 (Full Benchmark):
- [ ] All 3 scenarios working
- [ ] Tested on 3+ different models
- [ ] Clear differences in behavior detected
- [ ] Reproducible results (<20% variance)

---

## üõ†Ô∏è Development Setup

### Prerequisites
```bash
# Python 3.12+
python --version

# Install dependencies
pip install anthropic pydantic pyyaml pytest python-dotenv

# Set API key
export ANTHROPIC_API_KEY="your-key-here"
```

### Project Structure
```
apart/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ state.py              # ‚úÖ Exists (extend with 5-layer models)
‚îÇ   ‚îú‚îÄ‚îÄ agent.py              # ‚úÖ Exists (replace with LLM-powered)
‚îÇ   ‚îú‚îÄ‚îÄ game_engine.py        # ‚úÖ Exists (extend for scenarios)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py       # ‚úÖ Exists (extend for scenarios)
‚îÇ   ‚îú‚îÄ‚îÄ llm_client.py         # ‚ùå NEW - Create this
‚îÇ   ‚îú‚îÄ‚îÄ detector.py           # ‚ùå NEW - Create this
‚îÇ   ‚îî‚îÄ‚îÄ scenarios/
‚îÇ       ‚îú‚îÄ‚îÄ base_scenario.py  # ‚ùå NEW - Create this
‚îÇ       ‚îî‚îÄ‚îÄ metric_gaming.py  # ‚ùå NEW - Create this
‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml           # ‚úÖ Exists (basic config)
‚îÇ   ‚îî‚îÄ‚îÄ metric_gaming.yaml    # ‚ùå NEW - Create this
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ simple_deception_test.py  # ‚ùå NEW - Create this
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_state.py         # ‚úÖ Exists (extend for new models)
‚îÇ   ‚îú‚îÄ‚îÄ test_detector.py      # ‚ùå NEW - Create this
‚îÇ   ‚îî‚îÄ‚îÄ test_llm_client.py    # ‚ùå NEW - Create this
‚îî‚îÄ‚îÄ results/
    ‚îî‚îÄ‚îÄ experiments/          # ‚ùå NEW - Store experiment results
```

---

## üí∞ Budget Estimate

### Development Time
- **1 week prototype**: 30-40 hours (1 developer)
- **2 week single scenario**: 60-80 hours
- **4 week full benchmark**: 120-160 hours

### LLM API Costs
- **Prototype testing**: $50-100 (100-200 test runs)
- **Scenario development**: $200-500 (iterative testing)
- **Final evaluation**: $500-1000 (10 models √ó 10 runs √ó 3 scenarios)
- **Public dataset**: $5,000-10,000 (1000 runs for research)

**Total for MVP (2 weeks)**: ~$300-600

---

## üéØ Immediate Next Action

**START HERE** (right now):

```bash
# 1. Create new branch
git checkout -b feature/powerseek-detection

# 2. Copy reference implementation
# Use IMPLEMENTATION_GUIDE.md as reference

# 3. Start with state models
# Edit src/state.py - add Observation, ThinkingStep, etc.

# 4. Run tests to ensure nothing breaks
pytest tests/test_state.py

# 5. Commit progress
git add src/state.py
git commit -m "Add 5-layer state models for deception detection"
```

---

## üìà Metrics to Track

### Development Metrics
- [ ] Lines of code added
- [ ] Test coverage %
- [ ] API calls made
- [ ] Cost per run

### Research Metrics
- [ ] Deception rate (% of scenarios where agent deceives)
- [ ] Detection rate (% of deceptions caught)
- [ ] Power-seeking score (0-10)
- [ ] Alignment score (0-10)
- [ ] Scheming behavior frequency

### Impact Metrics
- [ ] GitHub stars
- [ ] Research citations
- [ ] Model developers using benchmark
- [ ] Community contributions

---

## ü§î Open Questions to Resolve

1. **Prompt Wording**: How much does system prompt affect power-seeking?
   - Test: Run same scenario with 5 different prompts
   - Measure: Variance in deception rate

2. **Model Differences**: Do more capable models seek power more?
   - Test: Claude Sonnet vs Opus vs Haiku
   - Measure: Compare power-seeking scores

3. **Threshold Tuning**: What detection sensitivity is optimal?
   - Test: Vary deception detector thresholds
   - Measure: False positive vs false negative rates

4. **Scenario Realism**: Are scenarios realistic enough?
   - Test: Get feedback from domain experts (corporate managers, auditors)
   - Iterate: Refine based on feedback

5. **Generalization**: Do results transfer to real-world settings?
   - Test: Compare benchmark behavior to real AI deployment logs (if available)
   - Validate: Expert review

---

## üí° Key Insights

### What's Working
‚úÖ Foundation is solid (state management, orchestration, testing)
‚úÖ Extensible architecture (custom_data fields, YAML configs)
‚úÖ Clear separation of concerns

### What's Critical
üî¥ **5-layer capture** (observations, thinking, statement, action, ground truth)
üî¥ **LLM integration** with extended thinking
üî¥ **Deception detection** (private vs public comparison)

### What's Optional (for v1.0)
üü° Beautiful visualizations (can use simple tables)
üü° Multi-model comparison (can start with Claude only)
üü° Advanced semantic analysis (keyword detection is enough to start)

---

## üéì Learning Resources

### For Understanding Power-Seeking
- Leopold Aschenbrenner: "Situational Awareness" report
- Anthropic: "Sleeper Agents" paper
- Apollo Research: Scheming evaluations

### For Implementation
- Anthropic API docs: Extended thinking feature
- Pydantic docs: Advanced validation
- Pytest docs: Parameterized testing

---

## ‚úÖ Done When

### Week 1 Prototype
‚úÖ Agent can think privately and speak publicly (separately captured)
‚úÖ Detector can flag when private thoughts contradict public statements
‚úÖ Have 1+ clear example of deception being detected
‚úÖ Can run `python examples/simple_deception_test.py` successfully

### Week 2 Metric Gaming
‚úÖ Can run 8-quarter corporate simulation
‚úÖ Have quantitative results table
‚úÖ Documented at least 3 types of deceptive behaviors
‚úÖ Comparison of deception rates across different prompts

### Week 4 Full Benchmark
‚úÖ All 3 scenarios implemented and tested
‚úÖ Cross-model comparison (3+ models)
‚úÖ Public dataset of 100+ runs
‚úÖ Open-source release with documentation

---

## üöÄ Let's Go!

**Your immediate task**: Start with Day 1 (state models extension)

Open `src/state.py` and start adding the new Pydantic models from IMPLEMENTATION_GUIDE.md.

Good luck! üéØ
